{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, interpolate and dump of GPS data\n",
    "\n",
    "e.g. will build a file  `velocity_cat=custom_ASlag=45_MS=5.5.npy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Utils (MANDATORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seismicutils\n",
    "from seismicutils import SeismicUtils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "if(True):\n",
    "    reload(seismicutils)\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "import os\n",
    "import networkx as nx\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "# import warnings\n",
    "\n",
    "from numba import njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the folder where to put csv files of GPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rootpath=\"/home/flandes/csv/\"\n",
    "rootpath=\"/Users/vincenzo/gps_csv/\"\n",
    "#rootpath=\"D:/gps/\"\n",
    "if(not os.path.exists(rootpath)):\n",
    "    os.mkdir(rootpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catalog loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R0 = 6371  ## Earth's Radius\n",
    "R_search = 300   ## Radius (in km) for which we look for GPS stations\n",
    "## we predict aftershocks up to 45 days after the MS:\n",
    "aftershocks_time_window = np.timedelta64(45,'D') \n",
    "min_mainshock_mag = 6 # this is telling us the minimal mainshock magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_type = 'custom' # just a flag for the output file\n",
    "catalog_filename = 'custom_catalog.csv' # valid values: 'custom' or 'giuseppe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days_forward  = np.timedelta64(1,'D')  ## how many days of GPS data to use after the MS \n",
    "n_days_backward = np.timedelta64(8, 'D') ## how many days of GPS data to use before the MS\n",
    "n_total_days = 1 +  n_days_forward.astype('int')+  n_days_backward.astype('int')\n",
    "## total will be 4+1(MS day)+1\n",
    "n_total_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = seismicutils.return_regions() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_catalog = pd.read_csv(catalog_filename, sep=\" \", parse_dates=['datetime'])\n",
    "fit_catalog['day'] = fit_catalog.datetime.values.astype('datetime64[D]')\n",
    "fit_catalog.sort_values(by='day', inplace=True)\n",
    "fit_catalog.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mainshocks = fit_catalog[fit_catalog.type==1]\n",
    "#mainshocks.reset_index(inplace=True, drop=True)\n",
    "#drop_ids = set()\n",
    "#for i in range(0, len(mainshocks)):\n",
    "#    if(mainshocks.iloc[i].seq_id in drop_ids):\n",
    "#        continue\n",
    "#    for j in range(i+1, len(mainshocks)):\n",
    "#        delta_days = (mainshocks.iloc[j].day.to_numpy() - mainshocks.iloc[i].day.to_numpy()).astype('timedelta64[D]')\n",
    "#        if(delta_days > aftershocks_time_window):\n",
    "#            break\n",
    "#        arg1 = np.radians(np.array([mainshocks.iloc[i].lat, mainshocks.iloc[i].lon]))[None,:]\n",
    "#        arg2 = np.radians(np.array([mainshocks.iloc[j].lat, mainshocks.iloc[j].lon]))[None,:]\n",
    "#        d_ij = R0*haversine_distances(arg1, arg2)[0,0]\n",
    "#        if(d_ij < R_search):\n",
    "#            print(mainshocks.iloc[j].seq_id,delta_days, d_ij, mainshocks.iloc[j].day.to_numpy())\n",
    "#            drop_ids.add(mainshocks.iloc[j].seq_id)\n",
    "#print('TO DROP SZ:', len(drop_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPS stations and Mainshocks (TO BE RUN ONLY THE FIRST TIME, OR IF SOMETHING CHANGED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the correct catalog (if the previous cell has run, it is already saved locally)\n",
    "\n",
    "This is useful if we previously run and saved the catalog part of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and plot stations (spatial distribution)\n",
    "We first download the list of all stations, then plot the distribution of the minimal station-to-station distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngl_list = SeismicUtils.get_ngl_stations(post_process=True)\n",
    "plot_station_distribution = True\n",
    "if(plot_station_distribution):\n",
    "    jlist = ngl_list[(ngl_list.lat>= regions['japan'][0])*(ngl_list.lat<= regions['japan'][1])\\\n",
    "                    *(ngl_list.lon >= regions['japan'][2])*(ngl_list.lon<= regions['japan'][3])]\n",
    "    jlist_distances = haversine_distances(np.radians(jlist[['lat','lon']].values))\n",
    "    plt.hist(R0*np.sort(jlist_distances, axis=0)[1:2,:].mean(axis=0), bins=np.linspace(0, 1e2))\n",
    "    plt.xlabel('Minimal distance $d$ (km)')\n",
    "    plt.ylabel('$P(d)$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "most stations have a neighboring one in a distance < 20km. \n",
    "\n",
    "So, $\\sigma_{interpolation}$ can be of the order of 20 km. \n",
    "\n",
    "For extrapolation (i.e. the choice of `alpha_max_dist`), one should take sthg of the order of 20km or a bit more (a bit more than  $\\sigma_{interpolation}$ for sure), but not much more than 20km."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each Mainshock, we find all stations around in a large radius (much larger than our future box). \n",
    "\n",
    "We discard MS that have too few stations (but not in a very restrictive way, for now)\n",
    "\n",
    "\"*Actual selection of mainshocks vs stations collection*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we ask for min_station_number to be there, but in a very large radius !\n",
    "## so we may further filter later\n",
    "min_station_number = 3\n",
    "stations_to_download = set()\n",
    "mainshock_stations = {}\n",
    "mainshock_day = {}\n",
    "for id, seq in fit_catalog.groupby('seq_id'):\n",
    "    if(len(seq) <= 1):\n",
    "        continue\n",
    "    mainshock = seq[seq['type'] == 1] ## finding one MS\n",
    "    expected_maximal_radius = max(10**(mainshock.mag.values[0]/2 - 0.79), R_search) ## still a radius\n",
    "    stations_to_ms_dist = haversine_distances(np.radians(ngl_list[[\"lat\",\"lon\"]]), np.radians(mainshock[[\"lat\",\"lon\"]].values))[:,0]\n",
    "    valid_stations_mask  =  R0*stations_to_ms_dist <= expected_maximal_radius\n",
    "    valid_stations_mask*= (ngl_list.begin.values <= mainshock.day.values[0])\\\n",
    "                           *(ngl_list.end.values >= mainshock.day.values[0]) ## discard stations that are not yet born or that have been terminated\n",
    "    valid_stations = ngl_list.name.values[valid_stations_mask]\n",
    "    \n",
    "    if(len(valid_stations) >= min_station_number):\n",
    "        print('Success: ', id, len(valid_stations), )\n",
    "        stations_to_download.update(list(valid_stations))\n",
    "        mainshock_stations[id] = list(valid_stations)\n",
    "        mainshock_day[id] = mainshock.day.values[0].astype('datetime64[D]')\n",
    "    else:\n",
    "        print('Failed: ', id, len(valid_stations), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load (or down-load) the csv (GPS data) files from NGL\n",
    "We download/load according to the mainshocks previously collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if(not os.path.exists(rootpath)):\n",
    "    os.mkdir(rootpath)\n",
    "mainshock_data = {}\n",
    "labels_to_rename = {\"_latitude(deg)\" : \"lat\" ,\"_longitude(deg)\" : \"lon\", \"__height(m)\": \"height\"}\n",
    "max_n_trials = 5\n",
    "for s in stations_to_download:\n",
    "    trial_fname = rootpath + s + \".csv\"\n",
    "    if(os.path.exists(trial_fname)):\n",
    "        print(s, \"(load existing)\")\n",
    "\n",
    "        data = pd.read_csv(trial_fname, sep=\" \", parse_dates=['date'])\n",
    "    else:\n",
    "        print(s, \"(download)\")\n",
    "        trials = 0\n",
    "        while(trials < max_n_trials):\n",
    "            try:\n",
    "                data = SeismicUtils.get_ngl_gps_data(s,\"IGS14\", \"tenv3\")\n",
    "                break\n",
    "            except:\n",
    "                trials += 1\n",
    "        if(trials == max_n_trials):\n",
    "            print(\"Failed. Tried: \", trials)\n",
    "            continue\n",
    "        data['date'] = [ seismicutils.SeismicUtils.str_to_datetime(s, 23) for s in data['YYMMMDD']]\n",
    "        data['date'] = data['date'].values.astype('datetime64[D]')\n",
    "        data.rename(labels_to_rename,axis=1, inplace=True)\n",
    "        data = data[['date','site','lat','lon','height']]\n",
    "        data.to_csv(trial_fname, sep=\" \", index=False)\n",
    "    data['lon'] = data['lon'] % 180\n",
    "    for id in mainshock_stations.keys():\n",
    "        if(s in mainshock_stations[id]):\n",
    "            subdata = data[(data.date >= mainshock_day[id] - n_days_backward)*(data.date <= mainshock_day[id] + n_days_forward)]\n",
    "            \n",
    "            if(len(subdata) == n_total_days and np.isfinite(subdata[['lat','lon','height']].values).all()):\n",
    "                if(id not in mainshock_data.keys()):\n",
    "                    mainshock_data[id] = []\n",
    "                mainshock_data[id].append(subdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the dictionnary  `fit_dataset`, which contains:\n",
    "(velocities, stations_positions, \n",
    "                           mainshock_day[id], mainshock.mag.values[0], \\\n",
    "                           mainshock_location, aftershocks_locations, aftershocks_mags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_factor = R0*1e3*np.pi/180  # convert from lat,lon (degrees) to meters\n",
    "fit_dataset = {}\n",
    "for id in mainshock_data:   ## sweep all MS that are m>6 (not much filtered)\n",
    "    velocities = []\n",
    "    stations_positions = []    \n",
    "    for md in mainshock_data[id]:  ## for each station close enough to that MS\n",
    "        days = md['date'].values\n",
    "        site = md['site'].values[0]\n",
    "        station_position = ngl_list[ngl_list.name == site][['lat','lon']].values[0,:]  # station position (initial one)\n",
    "        pos = md[['lat','lon','height']].values\n",
    "        vel = np.diff(pos, axis=0)  ## 1-day velocity : we take the diff of position between 2 days as signal\n",
    "        vel[:, [0,1]] = conv_factor*vel[:,[0,1]]\n",
    "        velocities.append(vel[:,np.newaxis, :])\n",
    "        stations_positions.append(station_position[np.newaxis, :])\n",
    "    velocities = np.concatenate(velocities, axis=1) ## concatenate 1-days velocities over all stations\n",
    "    ## velocities: (Tdays-1, Nstations(of that MS), 3)\n",
    "    stations_positions = np.concatenate(stations_positions)\n",
    "    \n",
    "    ## extraction of the AS\n",
    "    seq = fit_catalog[fit_catalog.seq_id==id]\n",
    "    mainshock   = seq[seq['type']==1]\n",
    "    aftershocks = seq[seq['type']==2]\n",
    "    aftershocks = aftershocks[aftershocks.day > mainshock.day.values[0] + n_days_forward]\n",
    "    ## note: at catalog extraction (before here) we already restricted to AS happening at t<t_MS+45 days.\n",
    "    mainshock_location    = mainshock  [['lat','lon']].values[0,:]\n",
    "    aftershocks_locations = aftershocks[['lat','lon']].values\n",
    "    aftershocks_mags = aftershocks['mag'].values\n",
    "    if(len(aftershocks_locations)>1):  #  and velocities.shape[0 (should be 1 and not 0)] >= min_station_number):\n",
    "        fit_dataset[id] = (velocities, stations_positions, \\\n",
    "                           mainshock_day[id], mainshock.mag.values[0], \\\n",
    "                           mainshock_location, aftershocks_locations, aftershocks_mags)\n",
    "## fit_dataset now contains all raw information relative to that sequence:\n",
    "## (lat,lon) coordinates, GPS data (at the stations, not interpolated)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists('velset')):\n",
    "    os.mkdir('velset')\n",
    "#temp = { k:fit_dataset[k] for k in fit_dataset.keys() if k not in drop_ids}\n",
    "#np.save('velset/' + SeismicUtils.format_velset_filename(catalog_type + 'filtered',aftershocks_time_window.astype('int'), min_mainshock_mag, n_total_days-1), temp)\n",
    "np.save('velset/' + SeismicUtils.format_velset_filename(catalog_type,aftershocks_time_window.astype('int'), min_mainshock_mag, n_total_days-1), fit_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "26830cff502e1cef407be67fc0678e858115322e66d911dee70bb6d4afc3944a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
