{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a velocity set (+labels), build a training set\n",
    "e.g. from  `velocity_cat=custom_ASlag=45_MS=5.5.npy` build `dataset_cat=custom_ASlag=45_MS=6_c=5_sigIt=8_days=2_minNStat=3.npy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seismicutils\n",
    "from seismicutils import SeismicUtils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt\n",
    "if(True):\n",
    "    reload(seismicutils)\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "import os\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import networkx as nx\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R0 = 6371  ## Earth's Radius\n",
    "R1 = 200   ## Lateral distance (in km) for which we expect AS to occur.\n",
    "## we predict aftershocks up to 45 days after the MS:\n",
    "aftershocks_time_window = np.timedelta64(45,'D') \n",
    "starting_date = np.datetime64('1995-01-01')  \n",
    "min_mainshock_mag = 6\n",
    "force_filter = True\n",
    "catalog_type = 'custom'\n",
    "nDays = 9\n",
    "name= 'velset/' +  SeismicUtils.format_velset_filename(catalog_type,aftershocks_time_window.astype('int'), min_mainshock_mag, nDays)\n",
    "fit_dataset = np.load(name, allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of days present in the data \n",
    "maximum_n_days = fit_dataset[list(fit_dataset.keys())[0]][0].shape[0]\n",
    "maximum_n_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "crucial parameters are:\n",
    "- cell_size_km\n",
    "- sigma_interpolation\n",
    "- min_aftershock_mag\n",
    "- n_days_from_end\n",
    "- min_stations_inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we looked for stations in a radius of R0=2*150 kms, the maximal size\n",
    "# of the window should be ~150*sqrt(2) km\n",
    "# in cell_size_km = 5 means 60 \\times 60 image\n",
    "cell_size_km = 5\n",
    "cell_size_rads = cell_size_km/R0\n",
    "cell_size_degs = cell_size_rads*180/np.pi\n",
    "scale_factor  = 1e3     ## from meters to mm (for the GPS data)\n",
    "sigma_interpolation = 8 ## expressed in number of cell sizes\n",
    "min_lateral_size = 25  # box latreal length will be 2 times this\n",
    "min_aftershock_mag = 4\n",
    "n_days_from_end = 2     ## you can further restrict the input data number of days\n",
    "min_stations_inside = 3 # minimal number of stations inside the image\n",
    "\n",
    "soft_labels = False      ## smoothed labels (from {0,1} to continuous, smoothed over space)\n",
    "sigma_softlabels = 1 ## expressed in number of cell sizes\n",
    "regression = False       ## labels are total magnitudes (m corresponding to sum of energies)\n",
    "use_constrained_fit = True  ## Thin 2D elastic sheet model for interpolating GPS data\n",
    "# Remark: the alpha_min will be chosen later, at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(fit_dataset.keys())  ## contains ordered sequences ids\n",
    "ids.sort()\n",
    "cartesian_dict = {}\n",
    "labels_dict = {}\n",
    "mainshock_info = {}\n",
    "minimal_distances = {}\n",
    "for id in ids:\n",
    "    fd = fit_dataset[id]\n",
    "    v, o, md,mm, ml, al, am = fd\n",
    "\n",
    "    \n",
    "\n",
    "    # locate a square region around the mainshock\n",
    "    min_lat, max_lat = ml[0]-min_lateral_size*cell_size_degs,ml[0]+min_lateral_size*cell_size_degs\n",
    "    min_lon, max_lon = ml[1]-min_lateral_size*cell_size_degs,ml[1]+min_lateral_size*cell_size_degs\n",
    "    n_discr_lat = 2*min_lateral_size\n",
    "    n_discr_lon = 2*min_lateral_size\n",
    "\n",
    "    # number of stations inside the region\n",
    "    n_stations_inside_image = ((o[:, 0] >= min_lat)*(o[:, 0] <= max_lat)\\\n",
    "                              *(o[:, 1] >= min_lon)*(o[:, 1] <= max_lon)).sum()\n",
    "\n",
    "    # don't proceed if not enough stations\n",
    "    if(n_stations_inside_image < min_stations_inside):\n",
    "        print('Skip:', id, 'with # stats:', n_stations_inside_image, 'of total (downloaded)', o.shape[0])\n",
    "        continue\n",
    "\n",
    "    od_stats = haversine_distances(o*np.pi/180).flatten()\n",
    "    od_stats = od_stats[od_stats>0]\n",
    "    minimal_distances[id] = R0*od_stats.min()\n",
    "\n",
    "    y_label = np.zeros((n_discr_lat, n_discr_lon))\n",
    "    # discretize AS locations to rows and columns indices\n",
    "    aftershocks_row = ((al[:,0]-min_lat)/cell_size_degs).astype('int')\n",
    "    aftershocks_col = ((al[:,1]-min_lon)/cell_size_degs).astype('int')\n",
    "    # mask to make sure no event is outside (overkill)\n",
    "    aftershocks_mask = aftershocks_row < n_discr_lat\n",
    "    aftershocks_mask *= aftershocks_row >= 0\n",
    "    aftershocks_mask *= aftershocks_col < n_discr_lon\n",
    "    aftershocks_mask *= aftershocks_col >= 0 \n",
    "    aftershocks_mask *= am >= min_aftershock_mag\n",
    "    \n",
    "    if(aftershocks_mask.sum() == 0):\n",
    "        continue\n",
    "    if(regression):\n",
    "        continue\n",
    "    else:\n",
    "        if(soft_labels):\n",
    "            SeismicUtils.create_soft_labels(y_label, aftershocks_row[aftershocks_mask], aftershocks_col[aftershocks_mask], cell_size_rads, sigma_softlabels*cell_size_rads)\n",
    "        else:\n",
    "            y_label[aftershocks_row[aftershocks_mask], aftershocks_col[aftershocks_mask]] = 1\n",
    "    \n",
    "    if (use_constrained_fit):\n",
    "        v0   = v.mean(axis=1)\n",
    "        v0_s = v.std (axis=1)\n",
    "        v_fit = (v - v0[:,None,:])/v0_s[:,None,:]\n",
    "    try:\n",
    "        out_arrays = []\n",
    "        for time_index in range(max(0, v.shape[0] - n_days_from_end),v.shape[0]):\n",
    "            if(use_constrained_fit):\n",
    "                automatic_reg_factor = 2 #minimal_distances[id]/cell_size_km\n",
    "\n",
    "                out_array, body_forces = seismicutils.fit_constrained( \n",
    "                    np.pi*o/180,\n",
    "                    v_fit[time_index,...],\n",
    "                    n_discr_lat, n_discr_lon,\n",
    "                    np.pi*min_lat/180,\n",
    "                    np.pi*min_lon/180,\n",
    "                    cell_size_rads,\n",
    "                    sigma_interpolation*cell_size_rads,\n",
    "                    reg_factor=automatic_reg_factor,\n",
    "                    index_ratio=0.8)\n",
    "                print('Success', id, time_index)\n",
    "                out_array[:,:,:2] *= v0_s[time_index,:2]\n",
    "                out_array[:,:,:2] += v0[time_index,:2]\n",
    "            else:\n",
    "                out_array = np.zeros((n_discr_lat, n_discr_lon, 4))*np.nan\n",
    "                ## where the real computation happens (using numba, jit)\n",
    "                seismicutils.fit_gps(out_array, \n",
    "                        np.pi*o/180,\n",
    "                        v[time_index,...],\n",
    "                        n_discr_lat, n_discr_lon,\n",
    "                        np.pi*min_lat/180,\n",
    "                        np.pi*min_lon/180,\n",
    "                        cell_size_rads,\n",
    "                        sigma_interpolation*cell_size_rads,\n",
    "                        min_w=0) # by putting min_w = 0 all the pixels are fit\n",
    "            out_arrays.append(out_array)\n",
    "        out_arrays = np.array(out_arrays)\n",
    "        # out_arrays[:,:,:,:-1] *= scale_factor we rescale later, if needed\n",
    "        cartesian_dict[id] = out_arrays\n",
    "        labels_dict[id] = y_label\n",
    "        mainshock_info[id] = (md, ml)\n",
    "    except:\n",
    "        print('Failed', id)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(False):\n",
    "    direction = 0\n",
    "    for id in ids:\n",
    "        if(id in cartesian_dict):\n",
    "            fd = fit_dataset[id]\n",
    "            v, o, md,mm, ml, al, am = fd\n",
    "            iData = cartesian_dict[id]\n",
    "            print(id)\n",
    "            fig, ax = plt.subplots(ncols=3, figsize=(16, 4))\n",
    "            im = ax[0].imshow(scale_factor*iData[-2, :, :, direction], cmap='seismic', interpolation='none')\n",
    "            ax[0].contour(labels_dict[id])\n",
    "            divider = make_axes_locatable(ax[0])\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "            ax[0].set_title(\"%f %f\" %(scale_factor*v[-2, :, direction].min(), scale_factor*v[-2, :, direction].max()))\n",
    "            im = ax[1].imshow(scale_factor*iData[-1, :, :, direction], cmap='seismic', interpolation='none')\n",
    "            divider = make_axes_locatable(ax[1])\n",
    "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
    "            ax[1].set_title(\"%f %f\" %(scale_factor*v[-1, :, direction].min(), scale_factor*v[-1, :, direction].max()))\n",
    "            ax[2].imshow(iData[0,:,:,-1], vmin=0, vmax=1)\n",
    "            plt.show()\n",
    "            #max_i = iData[-1, :, :, :2].max(axis=(0,1))\n",
    "            #max_v = v[-1, :, :2].max(axis=0)\n",
    "            #print(id,max_v, max_i,minimal_distances[id]/cell_size_km)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_constrained_fit):\n",
    "    n_spatial_components = 2\n",
    "    n_polar_components = 3 #radial + 2 cos/sin\n",
    "else:\n",
    "    n_spatial_components = 3 #include upward direction\n",
    "    n_polar_components = 5 #radial, cylindrical, 2 cos/sin, 1 sin\n",
    "\n",
    "#cartesian_data = np.zeros((len(cartesian_dict), n_days_from_end,n_discr_lat, n_discr_lon,  n_spatial_components))\n",
    "#polar_data = np.zeros((len(cartesian_dict), n_days_from_end,n_discr_lat, n_discr_lon,  n_polar_components))\n",
    "cartesian_data = []\n",
    "confidence_data = []\n",
    "polar_data = []\n",
    "label_data = []\n",
    "id_data = []\n",
    "day_data = []\n",
    "for id, packed_item in cartesian_dict.items():\n",
    "    cart_item = packed_item[...,:-1]*scale_factor\n",
    "    cartesian_data.append(cart_item[None,...])\n",
    "    \n",
    "    alpha_item = packed_item[...,-1]\n",
    "    confidence_data.append(alpha_item[-1])\n",
    "    \n",
    "    ## polar coordinates: (2D or 3D)\n",
    "    north_east_modulus = np.sqrt(cart_item[...,0]**2+cart_item[...,1]**2)\n",
    "    north_angular = cart_item[...,0]/north_east_modulus ## North/modulus  = opposed/adjacent = cos(angle)\n",
    "    east_angular  = cart_item[...,1]/north_east_modulus  ## East /modululs = ....   /adjacent = sin(angle)\n",
    "    if(use_constrained_fit): ## 2D\n",
    "        temp_polar_concat = np.concatenate([north_east_modulus[...,None]\\\n",
    "                                      ,north_angular[...,None],east_angular[...,None] ], axis=-1)\n",
    "        ## 0: radial\n",
    "        ## 1,2: angular\n",
    "        #polar_feat_names =[\"north_east_modulus\", \"north_angular\", \"east_angular\"]\n",
    "    else: ## 3D\n",
    "        upward_modulus = np.abs(cart_item[...,2])\n",
    "        upward_angular = cart_item[...,2]/np.linalg.norm(cart_item, axis=-1)  ## ~sign(up.down)/norm(3 axes)\n",
    "        temp_polar_concat = np.concatenate([north_east_modulus[...,None], upward_modulus[...,None], north_angular[...,None],\\\n",
    "                                      east_angular[...,None], upward_angular[...,None] ], axis=-1)\n",
    "        ## 0,1: radial\n",
    "        ## 2,3,4: angular\n",
    "        #polar_feat_names =[\"north_east_modulus\", \"upward_modulus\", \"north_angular\", \"east_angular\", \"upward_angular\"]\n",
    "    polar_data.append(temp_polar_concat[None,...])\n",
    "    \n",
    "    label_data.append(labels_dict[id][None,...])\n",
    "    id_data.append(id)\n",
    "    day_data.append(mainshock_info[id][0])\n",
    "\n",
    "cartesian_data = np.concatenate(cartesian_data)\n",
    "polar_data = np.concatenate(polar_data)\n",
    "label_data = np.concatenate(label_data)\n",
    "confidence_data = np.array(confidence_data)\n",
    "id_data = np.array(id_data)\n",
    "day_data = np.array(day_data)\n",
    "n_days = cartesian_data.shape[1]\n",
    "# we do this in the same cell to avoid running twice the below lines\n",
    "# first: we put move the 'time' (index=1) axis close to component axis (index=4)\n",
    "# second: we flatten the last two axis ('time', 'comp') and we move it in front of (pixel, pixel) axes, adapt to pytorch\n",
    "cartesian_data  = cartesian_data.transpose((0,2,3,1,4)) \n",
    "cartesian_data  = cartesian_data.reshape((cartesian_data.shape[:3] + (-1,))).transpose((0, 3, 1, 2)) \n",
    "polar_data = polar_data.transpose((0,2,3,1,4))\n",
    "polar_data = polar_data.reshape((polar_data.shape[:3] + (-1,))).transpose((0, 3, 1, 2))\n",
    "# polar_feat_names = still the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature indices\n",
    "north_components = np.arange(0, n_days*n_spatial_components, n_spatial_components)\n",
    "east_components  = np.arange(1, n_days*n_spatial_components, n_spatial_components)\n",
    "vector_components = np.vstack([north_components,\n",
    "                               east_components]).T.flatten()\n",
    "\n",
    "## scalar_components: relate to the cartesian representation (telling us how the object transforms under rotation)\n",
    "if(use_constrained_fit):  ## n_spatial_components==2\n",
    "    scalar_components = None\n",
    "else:\n",
    "    scalar_components = np.arange(2, n_days*n_spatial_components, n_spatial_components)\n",
    "    \n",
    "## compute radial and angular components:\n",
    "if(use_constrained_fit): ## n_spatial_components==2\n",
    "    # contains sqrt(n**2+e**2)\n",
    "    radial_components = np.arange(0, n_days*n_polar_components, n_polar_components)\n",
    "    # n/sqrt(n**2+e**2) and e/sqrt(n**2+e**2)\n",
    "    angular_components = np.vstack([np.arange(1, n_days*n_polar_components, n_polar_components), \n",
    "                                    np.arange(2, n_days*n_polar_components, n_polar_components)]).T.flatten()\n",
    "    # no radial/angular 3d, as the system is natively 2D\n",
    "    radial_3d_components = None\n",
    "    angular_3d_components = None\n",
    "#     polar_feat_names =[\"north_east_modulus\", \"north_angular\", \"east_angular\"]\n",
    "    \n",
    "else:\n",
    "    # sqrt(n**2+e**2), basically the same as 2D case above\n",
    "    radial_components = np.arange(0, n_days*n_polar_components, n_polar_components)\n",
    "    # n/sqrt(n**2+e**2) and e/sqrt(n**2+e**2)\n",
    "    angular_components = np.vstack([np.arange(2, n_days*n_polar_components, n_polar_components), \n",
    "                                    np.arange(3, n_days*n_polar_components, n_polar_components)]).T.flatten()\n",
    "    radial_3d_components = np.vstack( [np.arange(0, n_days*n_polar_components, n_polar_components),\n",
    "                                    np.arange(1, n_days*n_polar_components, n_polar_components)]).T.flatten()\n",
    "    angular_3d_components = np.vstack([np.arange(2, n_days*n_polar_components, n_polar_components), \n",
    "                                    np.arange(3, n_days*n_polar_components, n_polar_components),\n",
    "                                    np.arange(4, n_days*n_polar_components, n_polar_components)]).T.flatten()\n",
    "#     polar_feat_names =[\"north_east_modulus\", \"upward_modulus\", \"north_angular\", \"east_angular\", \"upward_angular\"]\n",
    "\n",
    "day_spatial_components = np.arange(0, n_days*n_spatial_components).reshape(n_days,n_spatial_components)\n",
    "day_polar_components   = np.arange(0, n_days*n_polar_components).reshape(n_days,n_polar_components)\n",
    "components_dict = {}\n",
    "components_dict['north'] = north_components\n",
    "components_dict['east']  = east_components\n",
    "components_dict['vector'] = vector_components\n",
    "\n",
    "## if 3D, otherwise it's None:\n",
    "components_dict['scalar'] = scalar_components\n",
    "\n",
    "## good for both 3D and 2D\n",
    "## in 3D, it gives the 2D subset\n",
    "## in 2D it is native\n",
    "components_dict['radial'] = radial_components\n",
    "components_dict['angular'] = angular_components\n",
    "\n",
    "## if 3D, otherwise it's None:\n",
    "components_dict['radial3d'] = radial_3d_components\n",
    "components_dict['angular3d'] = angular_3d_components\n",
    "\n",
    "## at the moment not used, however it can be used for visualization\n",
    "components_dict['day_cart'] = day_spatial_components\n",
    "components_dict['day_pol'] = day_polar_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "for rem_id in id_data:\n",
    "    fd = fit_dataset[rem_id]\n",
    "    v, o, md,mm, ml, al, am = fd\n",
    "    info.append([md, rem_id, mm, ml[0], ml[1], len(am[am>=min_aftershock_mag]) ])\n",
    "info = pd.DataFrame(info, columns=['day','seq_id', 'mag','lat','lon', 'n_as'])\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_distance_km = np.sqrt(2)*cell_size_km*min_lateral_size*2\n",
    "#drop_ids = set()\n",
    "#for i in range(0, len(info)):\n",
    "#    if(info.iloc[i].seq_id in drop_ids):\n",
    "#        continue\n",
    "#    for j in range(i+1, len(info)):\n",
    "#        delta_days = (info.iloc[j].day.to_numpy() - info.iloc[i].day.to_numpy()).astype('timedelta64[D]')\n",
    "#        if(delta_days > aftershocks_time_window):\n",
    "#            break\n",
    "#        arg1 = np.radians(np.array([info.iloc[i].lat, info.iloc[i].lon]))[None,:]\n",
    "#        arg2 = np.radians(np.array([info.iloc[j].lat, info.iloc[j].lon]))[None,:]\n",
    "#        d_ij = R0*haversine_distances(arg1, arg2)[0,0]\n",
    "#        if(d_ij <= exclude_distance_km):\n",
    "#            print(info.iloc[j].seq_id,delta_days, d_ij, info.iloc[j].day.to_numpy())\n",
    "#            drop_ids.add(info.iloc[j].seq_id)\n",
    "#info = info[~info.seq_id.isin(drop_ids)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = R0*haversine_distances(np.radians(info[['lat','lon']].values))\n",
    "distance_matrix[np.abs(info['day'].values[None,:]-info['day'].values[:,None]) > aftershocks_time_window] =0\n",
    "distance_matrix[distance_matrix > exclude_distance_km  ] = 0\n",
    "distance_matrix[np.diag_indices_from(distance_matrix)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "keep_in_manual_selection = [273, 275, 282, 297, 294, 317, 341, 344,376,378,392,409,422,426, 430,440, \n",
    "                            446, 454, 468,472,492,520,521,525,534,537,548]\n",
    "g = nx.from_numpy_array(distance_matrix)\n",
    "drop_manual = []\n",
    "for comp in nx.connected_components(g):\n",
    "    if(len(comp) > 1):\n",
    "        comp_df = info[info.index.isin(comp)].reset_index(drop=True)\n",
    "        comp_df['dt'] = np.insert(np.diff(comp_df.day), 0, 0)\n",
    "        comp_df['ranking'] = np.argsort(np.argsort(-comp_df['mag'].values))\n",
    "        comp_df['keep'] = np.zeros(len(comp_df), dtype=int)\n",
    "        comp_df.loc[comp_df.seq_id.isin(keep_in_manual_selection),'keep']=1\n",
    "        drop_manual.append(comp_df[~comp_df.seq_id.isin(keep_in_manual_selection)]['seq_id'].values)\n",
    "        display(comp_df)\n",
    "        #if(len(comp_df)>20):\n",
    "        #    plt.scatter(comp_df.lon, comp_df.lat, c=comp_df.ranking, s=10**(comp_df.mag/3))\n",
    "        #    for _, my_row in comp_df[comp_df.mag>6.5].iterrows():\n",
    "        #        plt.annotate('%i' % my_row.seq_id, (my_row.lon, my_row.lat), color='red')\n",
    "        #    plt.show()        \n",
    "drop_manual = np.concatenate(drop_manual)\n",
    "print('TO DROP', len(drop_manual), drop_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not os.path.exists('dataset')):\n",
    "    os.mkdir('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask = np.ones(len(id_data), dtype=bool)\n",
    "for to_drop_id in drop_manual:\n",
    "    keep_mask[id_data==to_drop_id]=False\n",
    "keep_mask = np.argwhere(keep_mask).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_polar = np.copy(polar_data[keep_mask])\n",
    "for radial_idx in components_dict['radial']:\n",
    "    sc_min  = np.min(scaled_polar[:, radial_idx,:,:], axis=(1,2))\n",
    "    sc_max  = np.max(scaled_polar[:, radial_idx,:,:], axis=(1,2))\n",
    "    scaled_polar[:, radial_idx,:,:] -= sc_min[:,None,None]\n",
    "    scaled_polar[:, radial_idx,:,:] /= (sc_max-sc_min)[:,None,None]\n",
    "target_filename = SeismicUtils.format_dataset_filename(catalog_type, aftershocks_time_window.astype('int'), min_mainshock_mag, cell_size_km, sigma_interpolation, n_days_from_end, min_stations_inside, soft_labels, regression, use_constrained_fit)\n",
    "dict_to_save = {'cartesian' : cartesian_data[keep_mask], \\\n",
    "                'polar' : polar_data[keep_mask], \\\n",
    "                'scpolar' : scaled_polar, \\\n",
    "                'label' : label_data[keep_mask], \\\n",
    "                'alpha' : confidence_data[keep_mask],\\\n",
    "                'id' : id_data[keep_mask], \\\n",
    "                'day' : day_data[keep_mask], \\\n",
    "                'components' : components_dict}\n",
    "np.save('dataset/' + target_filename, dict_to_save, allow_pickle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "26830cff502e1cef407be67fc0678e858115322e66d911dee70bb6d4afc3944a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
